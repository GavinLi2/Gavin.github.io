{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Main_Training_Codes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GavinLi2/Gavin.github.io/blob/master/Main_Training_Codes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77f4iJ0qW9Q0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %load ML_diplay.py\n",
        "\"\"\"\n",
        "机器学习建模\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "import pickle\n",
        "import lightgbm as lgb\n",
        "from  datetime import datetime, timedelta\n",
        "import random\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPjjg6Xvjz8Q",
        "colab_type": "code",
        "outputId": "e7b53c0a-9ce6-4661-8219-87b646b32b9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRzh-LOTk9KH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"/content/drive/My Drive/1 - Walmart M5 Accuracy\" \n",
        "os.chdir(path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPGw9UqzUT0g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import Union\n",
        "from tqdm.auto import tqdm as tqdm\n",
        "\n",
        "class WRMSSEEvaluator(object):\n",
        "    \n",
        "    group_ids = ( 'all_id', 'state_id', 'store_id', 'cat_id', 'dept_id', 'item_id',\n",
        "        ['state_id', 'cat_id'],  ['state_id', 'dept_id'], ['store_id', 'cat_id'],\n",
        "        ['store_id', 'dept_id'], ['item_id', 'state_id'], ['item_id', 'store_id'])\n",
        "\n",
        "    def __init__(self, \n",
        "                 train_df: pd.DataFrame, \n",
        "                 valid_df: pd.DataFrame, \n",
        "                 calendar: pd.DataFrame, \n",
        "                 prices: pd.DataFrame):\n",
        "        '''\n",
        "        intialize and calculate weights\n",
        "        '''\n",
        "        self.calendar = calendar\n",
        "        self.prices = prices\n",
        "        self.train_df = train_df\n",
        "        self.valid_df = valid_df\n",
        "        self.train_target_columns = [i for i in self.train_df.columns if i.startswith('d_')]\n",
        "        self.weight_columns = self.train_df.iloc[:, -28:].columns.tolist()\n",
        "\n",
        "        self.train_df['all_id'] = \"all\"\n",
        "\n",
        "        self.id_columns = [i for i in self.train_df.columns if not i.startswith('d_')]\n",
        "        self.valid_target_columns = [i for i in self.valid_df.columns if i.startswith('d_')]\n",
        "\n",
        "        if not all([c in self.valid_df.columns for c in self.id_columns]):\n",
        "            self.valid_df = pd.concat([self.train_df[self.id_columns], self.valid_df],\n",
        "                                      axis=1, \n",
        "                                      sort=False)\n",
        "        self.train_series = self.trans_30490_to_42840(self.train_df, \n",
        "                                                      self.train_target_columns, \n",
        "                                                      self.group_ids)\n",
        "        self.valid_series = self.trans_30490_to_42840(self.valid_df, \n",
        "                                                      self.valid_target_columns, \n",
        "                                                      self.group_ids)\n",
        "        self.weights = self.get_weight_df()\n",
        "        self.scale = self.get_scale()\n",
        "        self.train_series = None\n",
        "        self.train_df = None\n",
        "        self.prices = None\n",
        "        self.calendar = None\n",
        "\n",
        "    def get_scale(self):\n",
        "        '''\n",
        "        scaling factor for each series ignoring starting zeros\n",
        "        '''\n",
        "        scales = []\n",
        "        for i in tqdm(range(len(self.train_series))):\n",
        "            series = self.train_series.iloc[i].values\n",
        "            series = series[np.argmax(series!=0):]\n",
        "            scale = ((series[1:] - series[:-1]) ** 2).mean()\n",
        "            scales.append(scale)\n",
        "        return np.array(scales)\n",
        "    \n",
        "    def get_name(self, i):\n",
        "        '''\n",
        "        convert a str or list of strings to unique string \n",
        "        used for naming each of 42840 series\n",
        "        '''\n",
        "        if type(i) == str or type(i) == int:\n",
        "            return str(i)\n",
        "        else:\n",
        "            return \"--\".join(i)\n",
        "    \n",
        "    def get_weight_df(self) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        returns weights for each of 42840 series in a dataFrame\n",
        "        \"\"\"\n",
        "        day_to_week = self.calendar.set_index(\"d\")[\"wm_yr_wk\"].to_dict()\n",
        "        weight_df = self.train_df[[\"item_id\", \"store_id\"] + self.weight_columns].set_index(\n",
        "            [\"item_id\", \"store_id\"]\n",
        "        )\n",
        "        weight_df = (\n",
        "            weight_df.stack().reset_index().rename(columns={\"level_2\": \"d\", 0: \"value\"})\n",
        "        )\n",
        "        weight_df[\"wm_yr_wk\"] = weight_df[\"d\"].map(day_to_week)\n",
        "        weight_df = weight_df.merge(\n",
        "            self.prices, how=\"left\", on=[\"item_id\", \"store_id\", \"wm_yr_wk\"]\n",
        "        )\n",
        "        weight_df[\"value\"] = weight_df[\"value\"] * weight_df[\"sell_price\"]\n",
        "        weight_df = weight_df.set_index([\"item_id\", \"store_id\", \"d\"]).unstack(level=2)[\n",
        "            \"value\"\n",
        "        ]\n",
        "        weight_df = weight_df.loc[\n",
        "            zip(self.train_df.item_id, self.train_df.store_id), :\n",
        "        ].reset_index(drop=True)\n",
        "        weight_df = pd.concat(\n",
        "            [self.train_df[self.id_columns], weight_df], axis=1, sort=False\n",
        "        )\n",
        "        weights_map = {}\n",
        "        for i, group_id in enumerate(tqdm(self.group_ids, leave=False)):\n",
        "            lv_weight = weight_df.groupby(group_id)[self.weight_columns].sum().sum(axis=1)\n",
        "            lv_weight = lv_weight / lv_weight.sum()\n",
        "            for i in range(len(lv_weight)):\n",
        "                weights_map[self.get_name(lv_weight.index[i])] = np.array(\n",
        "                    [lv_weight.iloc[i]]\n",
        "                )\n",
        "        weights = pd.DataFrame(weights_map).T / len(self.group_ids)\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def trans_30490_to_42840(self, df, cols, group_ids, dis=False):\n",
        "        '''\n",
        "        transform 30490 sries to all 42840 series\n",
        "        '''\n",
        "        series_map = {}\n",
        "        for i, group_id in enumerate(tqdm(self.group_ids, leave=False, disable=dis)):\n",
        "            tr = df.groupby(group_id)[cols].sum()\n",
        "            for i in range(len(tr)):\n",
        "                series_map[self.get_name(tr.index[i])] = tr.iloc[i].values\n",
        "        return pd.DataFrame(series_map).T\n",
        "    \n",
        "    def get_rmsse(self, valid_preds) -> pd.Series:\n",
        "        '''\n",
        "        returns rmsse scores for all 42840 series\n",
        "        '''\n",
        "        score = ((self.valid_series - valid_preds) ** 2).mean(axis=1)\n",
        "        rmsse = (score / self.scale).map(np.sqrt)\n",
        "        return rmsse\n",
        "\n",
        "    def score(self, valid_preds: Union[pd.DataFrame, np.ndarray]) -> float:\n",
        "        assert self.valid_df[self.valid_target_columns].shape == valid_preds.shape\n",
        "\n",
        "        if isinstance(valid_preds, np.ndarray):\n",
        "            valid_preds = pd.DataFrame(valid_preds, columns=self.valid_target_columns)\n",
        "\n",
        "        valid_preds = pd.concat([self.valid_df[self.id_columns], valid_preds],\n",
        "                                axis=1, \n",
        "                                sort=False)\n",
        "        valid_preds = self.trans_30490_to_42840(valid_preds, \n",
        "                                                self.valid_target_columns, \n",
        "                                                self.group_ids, \n",
        "                                                True)\n",
        "        self.rmsse = self.get_rmsse(valid_preds)\n",
        "        self.contributors = pd.concat([self.weights, self.rmsse], \n",
        "                                      axis=1, \n",
        "                                      sort=False).prod(axis=1)\n",
        "        return np.sum(self.contributors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-ysiehTDEOp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_train_data(train_start=750,test_start=1800,is_train=True):\n",
        "    # 基本参数\n",
        "    PRICE_DTYPES = {\"store_id\": \"category\", \"item_id\": \"category\", \"wm_yr_wk\": \"int16\",\"sell_price\":\"float32\" }\n",
        "    CAL_DTYPES={\"event_name_1\": \"category\", \"event_name_2\": \"category\", \"event_type_1\": \"category\", \n",
        "            \"event_type_2\": \"category\", \"weekday\": \"category\", 'wm_yr_wk': 'int16', \"wday\": \"int16\",\n",
        "            \"month\": \"int8\", \"year\": \"int16\", \"snap_CA\": \"int8\", 'snap_TX': 'int8', 'snap_WI': 'int8' }\n",
        "\n",
        "    start_day = train_start if is_train else test_start\n",
        "    numcols = [f\"d_{day}\" for day in range(start_day,1914)]\n",
        "    catcols = ['id', 'item_id', 'dept_id','store_id', 'cat_id', 'state_id']\n",
        "    SALE_DTYPES = {numcol: \"float32\" for numcol in numcols} \n",
        "    SALE_DTYPES.update({col: \"category\" for col in catcols if col != \"id\"})\n",
        "\n",
        "    # 加载price数据\n",
        "    price_data = pd.read_csv('sell_prices.csv',dtype=PRICE_DTYPES)\n",
        "    # 加载cal数据\n",
        "    cal_data = pd.read_csv('calendar.csv',dtype=CAL_DTYPES)\n",
        "    # 加载sale数据\n",
        "    sale_data = pd.read_csv('sales_train_validation.csv',dtype=SALE_DTYPES,usecols=catcols+numcols)\n",
        "\n",
        "    # 类别标签转换\n",
        "    for col, col_dtype in PRICE_DTYPES.items():\n",
        "        if col_dtype == \"category\":\n",
        "            price_data[col] = price_data[col].cat.codes.astype(\"int16\") \n",
        "            #↑cat方法将category转换为CategoricalAccessor对象，codes方法以Series返回该对象的所有类名及对应索引，用于将类标签转为数值\n",
        "            price_data[col] -= price_data[col].min() #令类标签从0开始\n",
        "\n",
        "    cal_data[\"date\"] = pd.to_datetime(cal_data[\"date\"])\n",
        "    for col, col_dtype in CAL_DTYPES.items():\n",
        "        if col_dtype == \"category\":\n",
        "            cal_data[col] = cal_data[col].cat.codes.astype(\"int16\")\n",
        "            cal_data[col] -= cal_data[col].min()\n",
        "\n",
        "    for col in catcols:\n",
        "        if col != \"id\":\n",
        "            sale_data[col] = sale_data[col].cat.codes.astype(\"int16\")\n",
        "            sale_data[col] -= sale_data[col].min()\n",
        "\n",
        "    if not is_train:\n",
        "        for day in range(1913+1, 1913+ 2*28 +1):\n",
        "            sale_data[f\"d_{day}\"] = np.nan\n",
        "    # melt函数：将数据分为三部分：①id-like的列；②variables列：将指定列的列名，融合到一列中；③values列：将被融合的列的值，与融合后的列一一对应\n",
        "    sale_data = pd.melt(sale_data,\n",
        "            id_vars = catcols,\n",
        "            value_vars = [col for col in sale_data.columns if col.startswith(\"d_\")],\n",
        "            var_name = \"d\",\n",
        "            value_name = \"sales\")\n",
        "    sale_data = sale_data.merge(cal_data, on= \"d\", copy = False)\n",
        "    sale_data = sale_data.merge(price_data, on = [\"store_id\", \"item_id\", \"wm_yr_wk\"], copy = False)\n",
        "\n",
        "    # 处理时间特征\n",
        "    # 有的时间特征没有，通过datetime的方法自动生成\n",
        "    date_features = {\n",
        "            \"wday\": \"weekday\",\n",
        "            \"week\": \"weekofyear\",\n",
        "            \"month\": \"month\",\n",
        "            \"quarter\": \"quarter\",\n",
        "            \"year\": \"year\",\n",
        "            \"mday\": \"day\",}\n",
        "\n",
        "    for date_feat_name, date_feat_func in date_features.items():\n",
        "        if date_feat_name in sale_data.columns:\n",
        "            pass\n",
        "        else:\n",
        "            sale_data[date_feat_name] = getattr(sale_data[\"date\"].dt, date_feat_func).astype(\"int16\")\n",
        "\n",
        "    sale_data.drop([\"wm_yr_wk\", \"weekday\"],axis=1,inplace=True)\n",
        "\n",
        "    return sale_data\n",
        "\n",
        "\n",
        "def create_feature(sale_data, is_train=True, day=None):\n",
        "    # 可以在这里加入更多的特征抽取方法\n",
        "    # 获取7天前的数据，28天前的数据\n",
        "    lags = [7, 28]\n",
        "    lag_cols = [f\"lag_{lag}\" for lag in lags ]\n",
        "\n",
        "    # 如果是测试集只需要计算一天的特征，减少计算量\n",
        "    # 注意训练集和测试集特征生成要一致\n",
        "    if is_train:\n",
        "        for lag, lag_col in zip(lags, lag_cols):\n",
        "            sale_data[lag_col] = sale_data[[\"id\",\"sales\"]].groupby(\"id\")[\"sales\"].shift(lag)\n",
        "    else:\n",
        "        for lag, lag_col in zip(lags, lag_cols):\n",
        "            sale_data.loc[sale_data.date == day, lag_col] = sale_data.loc[sale_data.date ==day-timedelta(days=lag), 'sales'].values\n",
        "            # day变量是指需要预测的日期。这里直接取需要预测的日期对应的lag的数据，而没有用shift函数，减小了计算量（不用冗余地处理非预测日期的数据）  \n",
        "\n",
        "\n",
        "    # 将获取7天前的数据，28天前的数据做移动平均\n",
        "    wins = [7, 28]\n",
        "\n",
        "    if is_train:\n",
        "        for win in wins :\n",
        "            for lag,lag_col in zip(lags, lag_cols):\n",
        "                sale_data[f\"rmean_{lag}_{win}\"] = sale_data[[\"id\", lag_col]].groupby(\"id\")[lag_col].transform(lambda x : x.rolling(win).mean()).astype('float32')\n",
        "    else:\n",
        "        for win in wins:\n",
        "            for lag in lags:\n",
        "                # 取lag天前，窗口大小为win的日期的数据【i.e.取两个时间节点间的数据】\n",
        "                df_window = sale_data[(sale_data.date <= day-timedelta(days=lag)) & (sale_data.date > day-timedelta(days=lag+win))]\n",
        "                df_window_grouped = df_window.groupby(\"id\").agg({'sales':'mean'}).reindex(sale_data.loc[sale_data.date==day,'id'])['sales']\n",
        "                sale_data.loc[sale_data.date == day, f\"rmean_{lag}_{win}\"] = df_window_grouped.astype('float32').values\n",
        "    return sale_data\n",
        "\n",
        "\n",
        "def train_model(train_data,valid_data):\n",
        "    params = {\"objective\" : \"tweedie\",\n",
        "              \"metric\" :\"rmse\",\n",
        "              \"seed\" : 666,\n",
        "              \"force_row_wise\" : True,\n",
        "              \"learning_rate\" : 0.075,\n",
        "              \"sub_feature\" : 0.8,\n",
        "              \"sub_row\" : 0.75,\n",
        "              \"bagging_freq\" : 1,\n",
        "              \"lambda_l2\" : 0.1,\n",
        "              \"nthread\": 8,\n",
        "              \"tweedie_variance_power\":1.2,\n",
        "              'verbosity': 1,\n",
        "              'num_iterations' : 1500,\n",
        "              'num_leaves': 128,\n",
        "              \"min_data_in_leaf\": 104,\n",
        "              'early_stopping_rounds' : 100\n",
        "              }\n",
        "\n",
        "    m_lgb = lgb.train(params, train_data, valid_sets = [train_data, valid_data], verbose_eval=50)\n",
        "\n",
        "    return m_lgb\n",
        "\n",
        "\n",
        "def predict_ensemble(train_cols,m_lgb):\n",
        "    date = datetime(2016,4,25) \n",
        "    # 选择要乘以的系数\n",
        "    alphas = [1.035, 1.03, 1.025]\n",
        "    weights = [1/len(alphas)]*len(alphas)\n",
        "    sub = 0.\n",
        "\n",
        "    test_data = create_train_data(is_train=False)\n",
        "\n",
        "    for icount, (alpha, weight) in enumerate(zip(alphas, weights)):\n",
        "\n",
        "        test_data_c = test_data.copy()\n",
        "        cols = [f\"F{i}\" for i in range(1,29)]\n",
        "\n",
        "\n",
        "        for i in range(0, 28):\n",
        "            day = date + timedelta(days=i)\n",
        "            print(i, day)\n",
        "            tst = test_data_c[(test_data_c.date >= day - timedelta(days=57)) & (test_data_c.date <= day)].copy()\n",
        "            tst = create_feature(tst,is_train=False, day=day)\n",
        "            tst = tst.loc[tst.date == day , train_cols]\n",
        "            test_data_c.loc[test_data_c.date == day, \"sales\"] = alpha*m_lgb.predict(tst)\n",
        "\n",
        "        # 改为提交数据的格式\n",
        "        test_sub = test_data_c.loc[test_data_c.date >= date, [\"id\", \"sales\"]].copy()\n",
        "        test_sub[\"F\"] = [f\"F{rank}\" for rank in test_sub.groupby(\"id\")[\"id\"].cumcount()+1]\n",
        "        test_sub = test_sub.set_index([\"id\", \"F\"]).unstack()[\"sales\"][cols].reset_index()\n",
        "        test_sub.fillna(0., inplace = True)\n",
        "        test_sub.sort_values(\"id\", inplace = True)\n",
        "        test_sub.reset_index(drop=True, inplace = True)\n",
        "        # test_sub.to_csv(f\"submission_{icount}.csv\",index=False)\n",
        "        if icount == 0 :\n",
        "            sub = test_sub\n",
        "            sub[cols] *= weight\n",
        "        else:\n",
        "            sub[cols] += test_sub[cols]*weight\n",
        "        print(icount, alpha, weight)\n",
        "    \n",
        "    sub2 = sub.copy()\n",
        "    # 把大于28天后的validation替换成evaluation\n",
        "    sub2[\"id\"] = sub2[\"id\"].str.replace(\"validation$\", \"evaluation\")\n",
        "    sub = pd.concat([sub, sub2], axis=0, sort=False)\n",
        "    sub.to_csv(\"submission_v13.csv\",index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTx9erJR-Jhm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 辅助函数\n",
        "# 1.内存优化\n",
        "def reduce_mem_usage(df, verbose=True):\n",
        "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "    start_mem = df.memory_usage().sum() / 1024**2    \n",
        "    for col in df.columns:\n",
        "        if col != 'sales':\n",
        "            col_type = df[col].dtypes\n",
        "            if col_type in numerics:\n",
        "                c_min = df[col].min()\n",
        "                c_max = df[col].max()\n",
        "                if str(col_type)[:3] == 'int':\n",
        "                    if c_min >= np.iinfo(np.uint8).min and c_max <= np.iinfo(np.uint8).max:\n",
        "                        df[col] = df[col].astype(np.uint8)\n",
        "                    elif c_min >= np.iinfo(np.uint16).min and c_max <= np.iinfo(np.uint16).max:\n",
        "                        df[col] = df[col].astype(np.uint16)\n",
        "                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                        df[col] = df[col].astype(np.int32)\n",
        "                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                        df[col] = df[col].astype(np.int64)  \n",
        "                else:\n",
        "                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                        df[col] = df[col].astype(np.float16)\n",
        "                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                        df[col] = df[col].astype(np.float32)\n",
        "                    else:\n",
        "                        df[col] = df[col].astype(np.float64)    \n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
        "    return df\n",
        "\n",
        "# 2.RMSSE评估\n",
        "with open('/content/drive/My Drive/1 - Walmart M5 Accuracy/Evaluation/submission_index.pkl','rb') as f:\n",
        "    submission_index = pickle.load(f)\n",
        "with open('/content/drive/My Drive/1 - Walmart M5 Accuracy/Evaluation/valid_id.pkl','rb') as f:\n",
        "    valid_pred = pickle.load(f)\n",
        "with open('/content/drive/My Drive/1 - Walmart M5 Accuracy/Evaluation/evaluate.pkl','rb') as f:\n",
        "    e = pickle.load(f)\n",
        "def evaluate(model,valid_sets,valid_pred=valid_pred,e=e,submission_index=submission_index,is_early_stopping=False):\n",
        "    if is_early_stopping:\n",
        "        valid_pred['sales'] = model.predict(valid_sets,num_iteration=model.best_iteration)\n",
        "    else:\n",
        "        valid_pred['sales'] = model.predict(valid_sets)\n",
        "    valid_pred = valid_pred.set_index(['id','d']).unstack()['sales']\n",
        "    valid_pred = valid_pred.reindex(submission_index).values\n",
        "    score = e.score(valid_pred)\n",
        "    return score\n",
        "\n",
        "# 3.Seed\n",
        "def seed_all(seed=666):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "# 4.FI Test\n",
        "def fi_test(model,valid_sets,base,test_cols=None,is_early_stopping=False):\n",
        "    print('Base WRMSSE:',base)\n",
        "    print('-'*5,'Permutation Features Importance','-'*5)\n",
        "    if test_cols:\n",
        "        for col in test_cols:\n",
        "            valid = valid_sets.copy() \n",
        "            valid[col] = np.random.permutation(valid[col].values)\n",
        "            permutated = evaluate(model,valid,is_early_stopping=is_early_stopping)\n",
        "            print(col,np.round(base - permutated, 5))\n",
        "    else:\n",
        "        for col in valid_sets.columns:\n",
        "            valid = valid_sets.copy() \n",
        "            valid[col] = np.random.permutation(valid[col].values)\n",
        "            permutated = evaluate(model,valid,is_early_stopping=is_early_stopping)\n",
        "            print(col,np.round(base - permutated, 5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJCEi5gL1aAH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed_all(666)\n",
        "\n",
        "sale_data = create_train_data(train_start=1800,is_train=True)\n",
        "sale_data = create_feature(sale_data)\n",
        "sale_data = reduce_mem_usage(sale_data)\n",
        "\n",
        "cat_feats = ['item_id', 'dept_id','store_id', 'cat_id', 'state_id'] + [\"event_name_1\", \"event_type_1\", \"event_name_2\", 'event_type_2'] + ['before_events','after_events']\n",
        "useless_cols = [\"id\", \"date\", \"sales\", \"d\", \"Weight\"]\n",
        "train_cols = sale_data.columns[~sale_data.columns.isin(useless_cols)]\n",
        "\n",
        "# valid set中有8个新品，存在nan值。为了便于后续WRMSSE计算，先提取验证集再清洗nan【lgb可以处理nan值】\n",
        "X_valid = sale_data.loc[(sale_data.date <= '2016-04-24') & (sale_data.date > '2016-03-27'),train_cols]\n",
        "y_valid = sale_data.loc[(sale_data.date <= '2016-04-24') & (sale_data.date > '2016-03-27'),'sales']\n",
        "\n",
        "# 清洗数据，选择需要训练的数据\n",
        "sale_data.dropna(inplace=True)\n",
        "\n",
        "X_train = sale_data.loc[sale_data.date <= '2016-03-27',train_cols]\n",
        "y_train = sale_data.loc[sale_data.date <= '2016-03-27',\"sales\"]\n",
        "train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=cat_feats, free_raw_data=True)\n",
        "\n",
        "valid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=cat_feats, reference=train_data, free_raw_data=True) \n",
        "\n",
        "del sale_data\n",
        "gc.collect()\n",
        "\n",
        "m_lgb = train_model(train_data,valid_data)\n",
        "joblib.dump(m_lgb,'lgb_v13.pkl')\n",
        "\n",
        "# 实例化评估对象\n",
        "train_df = pd.read_csv('sales_train_validation.csv')\n",
        "cal_data = pd.read_csv('calendar.csv')\n",
        "price_data = pd.read_csv('sell_prices.csv')\n",
        "train_fold_df = train_df.iloc[:,:-28]\n",
        "valid_fold_df = train_df.iloc[:,-28:].copy()\n",
        "e = WRMSSEEvaluator(train_fold_df, valid_fold_df, cal_data, price_data)\n",
        "del train_fold_df, train_df, cal_data, price_data\n",
        "\n",
        "m_lgb.eval_valid(evaluate_wrmsse)\n",
        "\n",
        "predict_ensemble(train_cols,m_lgb)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7hfsws798xx",
        "colab_type": "code",
        "outputId": "a5a697c1-c597-4ea2-a427-178f37571525",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        }
      },
      "source": [
        "base = evaluate(m_lgb,X_valid)\n",
        "fi_test(m_lgb,X_valid,base)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Base WRMSSE: 0.4291943513152775\n",
            "----- Permutation Features Importance -----\n",
            "item_id -0.71765\n",
            "dept_id -0.0816\n",
            "store_id -0.1077\n",
            "cat_id -0.02091\n",
            "state_id -0.01364\n",
            "wday -0.4683\n",
            "month -0.00107\n",
            "year 0.0\n",
            "event_name_1 0.0\n",
            "event_type_1 0.0\n",
            "event_name_2 0.0\n",
            "event_type_2 0.0\n",
            "snap_CA -0.00278\n",
            "snap_TX -0.00581\n",
            "snap_WI -0.03335\n",
            "before_events 0.0\n",
            "after_events -0.00841\n",
            "sell_price -0.0445\n",
            "week -0.00399\n",
            "quarter -0.00028\n",
            "mday -0.09072\n",
            "lag_7 -0.25349\n",
            "lag_28 -0.00464\n",
            "rmean_7_7 -0.24318\n",
            "rmean_28_7 -0.55458\n",
            "rmean_7_28 -0.41447\n",
            "rmean_28_28 -0.06325\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apy-r-vc7Yqz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "本单元格及之后的单元格均为特征工程和效果测试，以create_train_data生成的数据为base\n",
        "'''\n",
        "seed_all()\n",
        "sale_data = create_train_data(1800)\n",
        "# sale_data = create_train_data(550) # 由于lag56±28会减小更多样本，所以适当补充一些回来\n",
        "USELESS_COLS = [\"id\", \"date\", \"sales\", \"d\"]\n",
        "TARGET = 'sales'\n",
        "CAT_COLS = ['item_id', 'dept_id','store_id', 'cat_id', 'state_id','event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
        "TRAIN_COLS = sale_data.columns[~sale_data.columns.isin(USELESS_COLS)].tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2ov1mQsmIEx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 训练集\n",
        "X_train = sale_data.loc[sale_data.date <= '2016-03-27',TRAIN_COLS]\n",
        "y_train = sale_data.loc[sale_data.date <= '2016-03-27',TARGET]\n",
        "train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=CAT_COLS, free_raw_data=True)\n",
        "\n",
        "# 测试集\n",
        "X_valid = sale_data.loc[(sale_data.date <= '2016-04-24') & (sale_data.date > '2016-03-27'),TRAIN_COLS]\n",
        "y_valid = sale_data.loc[(sale_data.date <= '2016-04-24') & (sale_data.date > '2016-03-27'),TARGET]\n",
        "valid_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data, categorical_feature=CAT_COLS, free_raw_data=True)\n",
        "\n",
        "# 建模\n",
        "lgb_base = train_model(train_data,valid_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66RnW6nqggw8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "1. Target Encoding （CV正则化）\n",
        "\n",
        "结果：效果不佳，CV的target encoding有点过拟合，且用encoded features替换原features后，WRMSSE崩到了0.9+\n",
        "'''\n",
        "from sklearn import base\n",
        "from sklearn.model_selection import KFold\n",
        "class KFoldTargetEncoderTrain(base.BaseEstimator, base.TransformerMixin):\n",
        "\n",
        "    def __init__(self,colnames,targetName,n_fold=5,transDtypes=True,verbosity=True):\n",
        "\n",
        "        self.colnames = colnames\n",
        "        self.targetName = targetName\n",
        "        self.n_fold = n_fold\n",
        "        self.verbosity = verbosity\n",
        "        self.transDtypes = transDtypes\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "\n",
        "    def transform(self,X):\n",
        "\n",
        "        assert(type(self.targetName) == str)\n",
        "        assert(type(self.colnames) == str)\n",
        "        assert(self.colnames in X.columns)\n",
        "        assert(self.targetName in X.columns)\n",
        "\n",
        "        mean_of_target = X[self.targetName].mean()\n",
        "        kf = KFold(n_splits = self.n_fold, shuffle = False, random_state=666)\n",
        "\n",
        "\n",
        "\n",
        "        col_mean_name = self.colnames + '_' + 'Kfold_Target_Enc'\n",
        "        X[col_mean_name] = np.nan\n",
        "\n",
        "        for tr_ind, val_ind in kf.split(X):\n",
        "            X_tr, X_val = X.iloc[tr_ind], X.iloc[val_ind]\n",
        "            X.loc[X.index[val_ind], col_mean_name] = X_val[self.colnames].map(X_tr.groupby(self.colnames)[self.targetName].mean())\n",
        "\n",
        "        X[col_mean_name].fillna(mean_of_target, inplace = True)\n",
        "        \n",
        "        if self.transDtypes:\n",
        "            X[col_mean_name] = X[col_mean_name].astype('float16')\n",
        "\n",
        "        if self.verbosity:\n",
        "\n",
        "            encoded_feature = X[col_mean_name].values\n",
        "            print('Correlation between the new feature, {} and, {} is {}.'.format(col_mean_name,\n",
        "                                                    self.targetName,\n",
        "                                                    np.corrcoef(X[self.targetName].values,\n",
        "                                                    encoded_feature)[0][1]))\n",
        "        return X\n",
        "\n",
        "class KFoldTargetEncoderTest(base.BaseEstimator, base.TransformerMixin):\n",
        "    \n",
        "    def __init__(self,train,colNames,encodedName,transDtypes=True):\n",
        "        \n",
        "        self.train = train\n",
        "        self.colNames = colNames\n",
        "        self.encodedName = encodedName\n",
        "        self.transDtypes = transDtypes\n",
        "        \n",
        "        \n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self,X):\n",
        "\n",
        "\n",
        "        mean = self.train[[self.colNames,self.encodedName]].groupby(self.colNames).mean().reset_index() \n",
        "        \n",
        "        dd = {}\n",
        "        for index, row in mean.iterrows():\n",
        "            dd[row[self.colNames]] = row[self.encodedName]\n",
        "\n",
        "        \n",
        "        X[self.encodedName] = X[self.colNames]\n",
        "        X = X.replace({self.encodedName: dd})\n",
        "\n",
        "        if self.transDtypes:\n",
        "            X[self.encodedName] = X[self.encodedName].astype('float16')\n",
        "        \n",
        "        return X\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMOu4btyfabN",
        "colab_type": "code",
        "outputId": "21af82a9-6bba-4ca6-832c-d3168f0407c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "train_data = sale_data.loc[sale_data.date <= '2016-03-27',TRAIN_COLS+[TARGET]]\n",
        "valid_data = sale_data.loc[(sale_data.date <= '2016-04-24') & (sale_data.date > '2016-03-27'),TRAIN_COLS+[TARGET]]\n",
        "\n",
        "enc_cols = ['item_id', 'dept_id', 'store_id', 'cat_id', 'state_id','event_name_1', 'event_type_1']\n",
        "\n",
        "for col in enc_cols:\n",
        "    enc = KFoldTargetEncoderTrain(colnames=col, targetName=TARGET,n_fold=3)\n",
        "    train_data = enc.fit_transform(train_data)\n",
        "    enc_test = KFoldTargetEncoderTest(train_data,col,col+'_Kfold_Target_Enc')\n",
        "    valid_data = enc_test.fit_transform(valid_data)\n",
        "    TRAIN_COLS.append(col+'_Kfold_Target_Enc')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correlation between the new feature, item_id_Kfold_Target_Enc and, sales is 0.5942211894991276.\n",
            "Correlation between the new feature, dept_id_Kfold_Target_Enc and, sales is 0.18570158365370654.\n",
            "Correlation between the new feature, store_id_Kfold_Target_Enc and, sales is 0.0832077315897998.\n",
            "Correlation between the new feature, cat_id_Kfold_Target_Enc and, sales is 0.13470398722252855.\n",
            "Correlation between the new feature, state_id_Kfold_Target_Enc and, sales is 0.01659626421855363.\n",
            "Correlation between the new feature, event_name_1_Kfold_Target_Enc and, sales is 0.008493968512612545.\n",
            "Correlation between the new feature, event_type_1_Kfold_Target_Enc and, sales is -0.009527891981037954.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gF9pofPWhJSR",
        "colab_type": "code",
        "outputId": "2c60c928-e973-4e08-c64a-101b9db004a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        }
      },
      "source": [
        "train_data.drop(columns=enc_cols,axis=1,inplace=True)\n",
        "valid_data.drop(columns=enc_cols,axis=1,inplace=True)\n",
        "\n",
        "X_train = train_data[train_data.columns[train_data.columns != TARGET]]\n",
        "y_train = train_data[TARGET]\n",
        "\n",
        "X_valid = valid_data[train_data.columns[train_data.columns != TARGET]]\n",
        "y_valid = valid_data[TARGET]\n",
        "\n",
        "del train_data, valid_data, sale_data\n",
        "gc.collect()\n",
        "\n",
        "train_data = lgb.Dataset(X_train, label=y_train, free_raw_data=True)\n",
        "valid_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data, free_raw_data=True)\n",
        "\n",
        "lgb_encoded = train_model(train_data,valid_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[50]\ttraining's rmse: 2.89761\tvalid_1's rmse: 2.68914\n",
            "[100]\ttraining's rmse: 2.8161\tvalid_1's rmse: 2.65543\n",
            "[150]\ttraining's rmse: 2.77011\tvalid_1's rmse: 2.65396\n",
            "[200]\ttraining's rmse: 2.74229\tvalid_1's rmse: 2.64667\n",
            "[250]\ttraining's rmse: 2.71372\tvalid_1's rmse: 2.63903\n",
            "[300]\ttraining's rmse: 2.69055\tvalid_1's rmse: 2.64016\n",
            "[350]\ttraining's rmse: 2.67194\tvalid_1's rmse: 2.63621\n",
            "[400]\ttraining's rmse: 2.65854\tvalid_1's rmse: 2.63302\n",
            "[450]\ttraining's rmse: 2.64876\tvalid_1's rmse: 2.63094\n",
            "[500]\ttraining's rmse: 2.6369\tvalid_1's rmse: 2.63126\n",
            "[550]\ttraining's rmse: 2.62619\tvalid_1's rmse: 2.62899\n",
            "[600]\ttraining's rmse: 2.61475\tvalid_1's rmse: 2.62764\n",
            "[650]\ttraining's rmse: 2.60616\tvalid_1's rmse: 2.62795\n",
            "[700]\ttraining's rmse: 2.59513\tvalid_1's rmse: 2.62918\n",
            "[750]\ttraining's rmse: 2.58641\tvalid_1's rmse: 2.62877\n",
            "[800]\ttraining's rmse: 2.57878\tvalid_1's rmse: 2.62965\n",
            "[850]\ttraining's rmse: 2.56957\tvalid_1's rmse: 2.62845\n",
            "[900]\ttraining's rmse: 2.56347\tvalid_1's rmse: 2.62789\n",
            "[950]\ttraining's rmse: 2.55648\tvalid_1's rmse: 2.62584\n",
            "[1000]\ttraining's rmse: 2.54744\tvalid_1's rmse: 2.6248\n",
            "[1050]\ttraining's rmse: 2.54132\tvalid_1's rmse: 2.62443\n",
            "[1100]\ttraining's rmse: 2.53646\tvalid_1's rmse: 2.62508\n",
            "[1150]\ttraining's rmse: 2.53073\tvalid_1's rmse: 2.62505\n",
            "[1200]\ttraining's rmse: 2.52565\tvalid_1's rmse: 2.62393\n",
            "[1250]\ttraining's rmse: 2.51993\tvalid_1's rmse: 2.62637\n",
            "[1300]\ttraining's rmse: 2.51464\tvalid_1's rmse: 2.62763\n",
            "[1350]\ttraining's rmse: 2.50741\tvalid_1's rmse: 2.62946\n",
            "[1400]\ttraining's rmse: 2.50402\tvalid_1's rmse: 2.62778\n",
            "[1450]\ttraining's rmse: 2.49875\tvalid_1's rmse: 2.63056\n",
            "[1500]\ttraining's rmse: 2.49454\tvalid_1's rmse: 2.62938\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZdKtQrBvoKg",
        "colab_type": "code",
        "outputId": "a3933f23-9ca4-4833-996e-961c917f8c04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "score_encoded = evaluate(lgb_encoded,X_valid)\n",
        "print('WRMSSE OF ENCODED MODEL:',np.round(score_encoded,5))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WRMSSE OF ENCODED MODEL: 0.9311\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qDD0gZGhgRr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ===============================================\n",
        "# 尝试用smoothing来正则化\n",
        "def smoothing_target_encoder(df, df_test, column, target, weight=100):\n",
        "    mean = df[target].mean()\n",
        "\n",
        "    calculated_df = df.groupby(column)[target].agg(['count','mean'])\n",
        "    counts = calculated_df['count']\n",
        "    means = calculated_df['mean']\n",
        "\n",
        "    smoothed = (counts * means + weight * mean) / (counts + weight)\n",
        "\n",
        "    col_name = column+'_smooth_encoded' \n",
        "\n",
        "    df[col_name] = df[column].map(smoothed)\n",
        "    df_test[col_name] = df_test[column].map(smoothed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiLZKBKNj19m",
        "colab_type": "code",
        "outputId": "963cfdd2-bd65-49bb-d8d2-5c1f41a87076",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        }
      },
      "source": [
        "train_data = sale_data.loc[sale_data.date <= '2016-03-27',TRAIN_COLS+[TARGET]]\n",
        "valid_data = sale_data.loc[(sale_data.date <= '2016-04-24') & (sale_data.date > '2016-03-27'),TRAIN_COLS+[TARGET]]\n",
        "\n",
        "enc_cols = ['item_id', 'dept_id', 'store_id', 'cat_id', 'state_id', 'event_name_1', 'event_type_1']\n",
        "\n",
        "for col in enc_cols:\n",
        "    smoothing_target_encoder(train_data,valid_data,col,TARGET,1000)\n",
        "\n",
        "X_train = train_data[train_data.columns[train_data.columns != TARGET]]\n",
        "y_train = train_data[TARGET]\n",
        "\n",
        "X_valid = valid_data[train_data.columns[train_data.columns != TARGET]]\n",
        "y_valid = valid_data[TARGET]\n",
        "\n",
        "del train_data, valid_data, sale_data\n",
        "gc.collect()\n",
        "\n",
        "train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=CAT_COLS, free_raw_data=True)\n",
        "valid_data = lgb.Dataset(X_valid, label=y_valid, categorical_feature=CAT_COLS, reference=train_data, free_raw_data=True)\n",
        "\n",
        "lgb_encoded_v3 = train_model(train_data,valid_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 100 rounds.\n",
            "[50]\ttraining's rmse: 2.66005\tvalid_1's rmse: 2.39729\n",
            "[100]\ttraining's rmse: 2.55608\tvalid_1's rmse: 2.2543\n",
            "[150]\ttraining's rmse: 2.51022\tvalid_1's rmse: 2.21304\n",
            "[200]\ttraining's rmse: 2.47493\tvalid_1's rmse: 2.20018\n",
            "[250]\ttraining's rmse: 2.46121\tvalid_1's rmse: 2.1924\n",
            "[300]\ttraining's rmse: 2.44979\tvalid_1's rmse: 2.19067\n",
            "[350]\ttraining's rmse: 2.43498\tvalid_1's rmse: 2.1844\n",
            "[400]\ttraining's rmse: 2.41919\tvalid_1's rmse: 2.17851\n",
            "[450]\ttraining's rmse: 2.41233\tvalid_1's rmse: 2.17645\n",
            "[500]\ttraining's rmse: 2.40607\tvalid_1's rmse: 2.17532\n",
            "[550]\ttraining's rmse: 2.39767\tvalid_1's rmse: 2.17165\n",
            "[600]\ttraining's rmse: 2.38951\tvalid_1's rmse: 2.1695\n",
            "[650]\ttraining's rmse: 2.38149\tvalid_1's rmse: 2.17117\n",
            "[700]\ttraining's rmse: 2.37294\tvalid_1's rmse: 2.16998\n",
            "[750]\ttraining's rmse: 2.36039\tvalid_1's rmse: 2.16571\n",
            "[800]\ttraining's rmse: 2.3542\tvalid_1's rmse: 2.16136\n",
            "[850]\ttraining's rmse: 2.34693\tvalid_1's rmse: 2.15835\n",
            "[900]\ttraining's rmse: 2.34159\tvalid_1's rmse: 2.15636\n",
            "[950]\ttraining's rmse: 2.33647\tvalid_1's rmse: 2.15509\n",
            "[1000]\ttraining's rmse: 2.33063\tvalid_1's rmse: 2.15297\n",
            "[1050]\ttraining's rmse: 2.32567\tvalid_1's rmse: 2.1524\n",
            "[1100]\ttraining's rmse: 2.32041\tvalid_1's rmse: 2.15105\n",
            "[1150]\ttraining's rmse: 2.31565\tvalid_1's rmse: 2.14963\n",
            "[1200]\ttraining's rmse: 2.31095\tvalid_1's rmse: 2.14876\n",
            "[1250]\ttraining's rmse: 2.30546\tvalid_1's rmse: 2.14691\n",
            "[1300]\ttraining's rmse: 2.29927\tvalid_1's rmse: 2.14467\n",
            "[1350]\ttraining's rmse: 2.29576\tvalid_1's rmse: 2.14376\n",
            "[1400]\ttraining's rmse: 2.29154\tvalid_1's rmse: 2.14274\n",
            "[1450]\ttraining's rmse: 2.28816\tvalid_1's rmse: 2.14374\n",
            "Early stopping, best iteration is:\n",
            "[1388]\ttraining's rmse: 2.2926\tvalid_1's rmse: 2.14221\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCivkY6g9Egu",
        "colab_type": "code",
        "outputId": "3bf78162-7f54-482f-e1fe-26c258960094",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "score_encoded = evaluate(lgb_encoded_v3,X_valid,is_early_stopping=True)\n",
        "fi_test(lgb_encoded_v3, X_valid,score_encoded, enc_cols + [i+'_smooth_encoded' for i in enc_cols],True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Base WRMSSE: 0.545424321763537\n",
            "----- Permutation Features Importance -----\n",
            "item_id -0.91142\n",
            "dept_id -0.09455\n",
            "store_id -0.257\n",
            "cat_id -0.03645\n",
            "state_id -0.06378\n",
            "event_name_1 0.0\n",
            "event_type_1 0.0\n",
            "item_id_smooth_encoded -1.37805\n",
            "dept_id_smooth_encoded -0.20743\n",
            "store_id_smooth_encoded -0.21184\n",
            "cat_id_smooth_encoded -0.00703\n",
            "state_id_smooth_encoded -0.00861\n",
            "event_name_1_smooth_encoded 0.0\n",
            "event_type_1_smooth_encoded 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEZBQB8_yyVo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "2. lag14和lag28*2；lag14_7，lag14_28，lag_56_7*2，lag56_28*2\n",
        "'''\n",
        "def create_feature(sale_data, is_train=True, day=None):\n",
        "    lags = [14, 56]\n",
        "    lag_cols = [f\"lag_{lag}\" for lag in lags ]\n",
        "\n",
        "    # LAG\n",
        "    if is_train:\n",
        "        for lag, lag_col in zip(lags, lag_cols):\n",
        "            sale_data[lag_col] = sale_data[[\"id\",\"sales\"]].groupby(\"id\")[\"sales\"].shift(lag)\n",
        "    else:\n",
        "        for lag, lag_col in zip(lags, lag_cols):\n",
        "            sale_data.loc[sale_data.date == day, lag_col] = sale_data.loc[sale_data.date == day-timedelta(days=lag), 'sales'].values\n",
        "\n",
        "    # ROLLING\n",
        "    wins = [7,28]\n",
        "    for win in wins:\n",
        "        if is_train:\n",
        "            # 处理lag14天的特征\n",
        "            sale_data[f\"rmean_14_{win}\"] = sale_data[[\"id\", 'lag_14']].groupby(\"id\")['lag_14'].transform(lambda x : x.rolling(win).mean()).astype('float32')\n",
        "            # 处理lag56天的特征\n",
        "            sale_data[f'rmean_56_{win}_2'] = sale_data[['id','lag_56']].groupby('id')['lag_56'].transform(lambda x : x.rolling(1+win*2,center=True).mean()).astype('float32')\n",
        "        else:\n",
        "            df_window = sale_data[(sale_data.date <= day-timedelta(days=14)) & (sale_data.date > day-timedelta(days=14+win))]\n",
        "            df_window_grouped = df_window.groupby(\"id\").agg({'sales':'mean'}).reindex(sale_data.loc[sale_data.date==day,'id'])['sales']\n",
        "            sale_data.loc[sale_data.date == day, f\"rmean_14_{win}\"] = df_window_grouped.astype('float32').values\n",
        "            \n",
        "            df_window = sale_data[(sale_data.date <= day-timedelta(days=56-win)) & (sale_data.date > day-timedelta(days=56+win))]\n",
        "            df_window_grouped = df_window.groupby(\"id\").agg({'sales':'mean'}).reindex(sale_data.loc[sale_data.date==day,'id'])['sales']\n",
        "            sale_data.loc[sale_data.date == day, f'rmean_56_{win}_2'] = df_window_grouped.astype('float32').values\n",
        "\n",
        "    return sale_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuD3mI6bTboo",
        "colab_type": "code",
        "outputId": "7d4b07e5-79b4-4315-a062-afcd4aaddd7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        }
      },
      "source": [
        "sale_data = create_feature(sale_data)\n",
        "\n",
        "TRAIN_COLS = sale_data.columns[~sale_data.columns.isin(USELESS_COLS)]\n",
        "\n",
        "X_valid = sale_data.loc[(sale_data.date <= '2016-04-24') & (sale_data.date > '2016-03-27'),TRAIN_COLS]\n",
        "y_valid = sale_data.loc[(sale_data.date <= '2016-04-24') & (sale_data.date > '2016-03-27'),TARGET]\n",
        "\n",
        "sale_data.dropna(inplace=True)\n",
        "\n",
        "X_train = sale_data.loc[sale_data.date <= '2016-03-27',TRAIN_COLS]\n",
        "y_train = sale_data.loc[sale_data.date <= '2016-03-27',TARGET]\n",
        "train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=CAT_COLS, free_raw_data=True)\n",
        "valid_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data, categorical_feature=CAT_COLS, free_raw_data=True)\n",
        "\n",
        "# 建模\n",
        "lgb_timeFeats = train_model(train_data,valid_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[50]\ttraining's rmse: 2.47906\tvalid_1's rmse: 2.23541\n",
            "[100]\ttraining's rmse: 2.39683\tvalid_1's rmse: 2.19286\n",
            "[150]\ttraining's rmse: 2.36184\tvalid_1's rmse: 2.21447\n",
            "[200]\ttraining's rmse: 2.33042\tvalid_1's rmse: 2.22937\n",
            "[250]\ttraining's rmse: 2.30972\tvalid_1's rmse: 2.24418\n",
            "[300]\ttraining's rmse: 2.28953\tvalid_1's rmse: 2.26875\n",
            "[350]\ttraining's rmse: 2.27165\tvalid_1's rmse: 2.27747\n",
            "[400]\ttraining's rmse: 2.25811\tvalid_1's rmse: 2.29594\n",
            "[450]\ttraining's rmse: 2.24431\tvalid_1's rmse: 2.33312\n",
            "[500]\ttraining's rmse: 2.23359\tvalid_1's rmse: 2.40463\n",
            "[550]\ttraining's rmse: 2.2228\tvalid_1's rmse: 2.40661\n",
            "[600]\ttraining's rmse: 2.21464\tvalid_1's rmse: 2.41639\n",
            "[650]\ttraining's rmse: 2.20589\tvalid_1's rmse: 2.42957\n",
            "[700]\ttraining's rmse: 2.19847\tvalid_1's rmse: 2.42907\n",
            "[750]\ttraining's rmse: 2.19424\tvalid_1's rmse: 2.42919\n",
            "[800]\ttraining's rmse: 2.18835\tvalid_1's rmse: 2.42479\n",
            "[850]\ttraining's rmse: 2.18213\tvalid_1's rmse: 2.42193\n",
            "[900]\ttraining's rmse: 2.17726\tvalid_1's rmse: 2.41866\n",
            "[950]\ttraining's rmse: 2.17222\tvalid_1's rmse: 2.42494\n",
            "[1000]\ttraining's rmse: 2.16837\tvalid_1's rmse: 2.41849\n",
            "[1050]\ttraining's rmse: 2.16441\tvalid_1's rmse: 2.41826\n",
            "[1100]\ttraining's rmse: 2.15992\tvalid_1's rmse: 2.41896\n",
            "[1150]\ttraining's rmse: 2.1561\tvalid_1's rmse: 2.42014\n",
            "[1200]\ttraining's rmse: 2.15258\tvalid_1's rmse: 2.43335\n",
            "[1250]\ttraining's rmse: 2.14799\tvalid_1's rmse: 2.4357\n",
            "[1300]\ttraining's rmse: 2.14398\tvalid_1's rmse: 2.45418\n",
            "[1350]\ttraining's rmse: 2.14101\tvalid_1's rmse: 2.45434\n",
            "[1400]\ttraining's rmse: 2.13711\tvalid_1's rmse: 2.46005\n",
            "[1450]\ttraining's rmse: 2.13425\tvalid_1's rmse: 2.4512\n",
            "[1500]\ttraining's rmse: 2.13029\tvalid_1's rmse: 2.45078\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2Ky3x3q8SBO",
        "colab_type": "code",
        "outputId": "fbd2caba-659e-4119-a280-7744849d83f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "score_base = evaluate(lgb_timeFeats,X_valid)\n",
        "fi_test(lgb_timeFeats,X_valid,score_base,test_cols=['lag_14','lag_56','rmean_14_7','rmean_56_7_2','rmean_14_28','rmean_56_28_2'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Base WRMSSE: 1.3808371485701458\n",
            "----- Permutation Features Importance -----\n",
            "lag_14 0.04056\n",
            "lag_56 -0.00242\n",
            "rmean_14_7 0.3695\n",
            "rmean_56_7_2 0.40054\n",
            "rmean_14_28 0.45123\n",
            "rmean_56_28_2 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_WIcrNfYau1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(train_data,valid_data):\n",
        "    params = {\"objective\" : \"tweedie\",\n",
        "              \"metric\" :\"rmse\",\n",
        "              \"seed\" : 666,\n",
        "              \"force_row_wise\" : True,\n",
        "              \"learning_rate\" : 0.075,\n",
        "              \"sub_feature\" : 0.7,\n",
        "              \"sub_row\" : 0.75,\n",
        "              \"bagging_freq\" : 1,\n",
        "              \"lambda_l2\" : 0.3,\n",
        "              \"nthread\": 8,\n",
        "              \"tweedie_variance_power\":1.2,\n",
        "              'verbosity': 1,\n",
        "              'num_iterations' : 1500,\n",
        "              'num_leaves': 128,\n",
        "              \"min_data_in_leaf\": 104,\n",
        "              'early_stopping_rounds' : 100\n",
        "              }\n",
        "\n",
        "    m_lgb = lgb.train(params, train_data, valid_sets = [train_data, valid_data], verbose_eval=50)\n",
        "\n",
        "    return m_lgb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uixvbw64peB",
        "colab_type": "code",
        "outputId": "c29b7394-341d-4363-c8f7-5ce07d9dc639",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "sale_data = create_feature(sale_data)\n",
        "\n",
        "TRAIN_COLS = sale_data.columns[~sale_data.columns.isin(USELESS_COLS)]\n",
        "\n",
        "X_valid = sale_data.loc[(sale_data.date <= '2016-04-24') & (sale_data.date > '2016-03-27'),TRAIN_COLS]\n",
        "y_valid = sale_data.loc[(sale_data.date <= '2016-04-24') & (sale_data.date > '2016-03-27'),TARGET]\n",
        "\n",
        "sale_data.dropna(inplace=True)\n",
        "\n",
        "X_train = sale_data.loc[sale_data.date <= '2016-03-27',TRAIN_COLS]\n",
        "y_train = sale_data.loc[sale_data.date <= '2016-03-27',TARGET]\n",
        "train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=CAT_COLS, free_raw_data=True)\n",
        "valid_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data, categorical_feature=CAT_COLS, free_raw_data=True)\n",
        "\n",
        "del sale_data\n",
        "gc.collect()\n",
        "\n",
        "# 建模\n",
        "lgb_timeFeats_v2 = train_model(train_data,valid_data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training until validation scores don't improve for 100 rounds.\n",
            "[50]\ttraining's rmse: 2.47594\tvalid_1's rmse: 2.28678\n",
            "[100]\ttraining's rmse: 2.39835\tvalid_1's rmse: 2.2293\n",
            "[150]\ttraining's rmse: 2.36507\tvalid_1's rmse: 2.23695\n",
            "[200]\ttraining's rmse: 2.33503\tvalid_1's rmse: 2.2468\n",
            "Early stopping, best iteration is:\n",
            "[109]\ttraining's rmse: 2.39141\tvalid_1's rmse: 2.21778\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yd9434TyJIbO",
        "colab_type": "code",
        "outputId": "4976321d-b829-4563-dd93-b43913933e7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "score_timeFeats_v2 = evaluate(lgb_timeFeats_v2,X_valid)\n",
        "print('WRMSSE OF ENCODED MODEL:',np.round(score_timeFeats_v2,5))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WRMSSE OF ENCODED MODEL: 0.68308\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4fN2wcTR2ZH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMOtuOodR2lB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_feature(sale_data, is_train=True, day=None):\n",
        "    # 可以在这里加入更多的特征抽取方法\n",
        "    # 获取7天前的数据，28天前的数据\n",
        "    lags = [7, 28]\n",
        "    lag_cols = [f\"lag_{lag}\" for lag in lags ]\n",
        "\n",
        "    # 如果是测试集只需要计算一天的特征，减少计算量\n",
        "    # 注意训练集和测试集特征生成要一致\n",
        "    if is_train:\n",
        "        for lag, lag_col in zip(lags, lag_cols):\n",
        "            sale_data[lag_col] = sale_data[[\"id\",\"sales\"]].groupby(\"id\")[\"sales\"].shift(lag)\n",
        "    else:\n",
        "        for lag, lag_col in zip(lags, lag_cols):\n",
        "            sale_data.loc[sale_data.date == day, lag_col] = sale_data.loc[sale_data.date ==day-timedelta(days=lag), 'sales'].values\n",
        "            # day变量是指需要预测的日期。这里直接取需要预测的日期对应的lag的数据，而没有用shift函数，减小了计算量（不用冗余地处理非预测日期的数据）  \n",
        "\n",
        "\n",
        "    # 将获取7天前的数据，28天前的数据做移动平均\n",
        "    wins = [7, 28]\n",
        "\n",
        "    if is_train:\n",
        "        for win in wins :\n",
        "            for lag,lag_col in zip(lags, lag_cols):\n",
        "                # sale_data = pd.concat([sale_data,\n",
        "                #                   sale_data[['id', lag_col]].groupby('id')[lag_col].apply(lambda x: x.rolling(win).agg({\n",
        "                #                                             f\"rmean_{lag}_{win}\":'mean',\n",
        "                #                                             f\"rmedian_{lag}_{win}\":'median',\n",
        "                #                                             f\"rstd_{lag}_{win}\":'std',\n",
        "                #                                             f\"rmax_{lag}_{win}\":'max',\n",
        "                #                                             f\"rmin_{lag}_{win}\":'min'}))], axis=1)\n",
        "                sale_data[f\"rmean_{lag}_{win}\"] = sale_data[[\"id\", lag_col]].groupby(\"id\")[lag_col].transform(lambda x : x.rolling(win).mean()).astype('float32')\n",
        "                if lag == 28:\n",
        "                    sale_data[f\"rmedian_{lag}_{win}\"] = sale_data[[\"id\", lag_col]].groupby(\"id\")[lag_col].transform(lambda x : x.rolling(win).median()).astype('float32')\n",
        "                    sale_data[f\"rstd_{lag}_{win}\"] = sale_data[[\"id\", lag_col]].groupby(\"id\")[lag_col].transform(lambda x : x.rolling(win).std()).astype('float32')\n",
        "                    sale_data[f\"rmax_{lag}_{win}\"] = sale_data[[\"id\", lag_col]].groupby(\"id\")[lag_col].transform(lambda x : x.rolling(win).max()).astype('float32')\n",
        "                else:\n",
        "                    pass\n",
        "    else:\n",
        "        for win in wins:\n",
        "            for lag in lags:\n",
        "                # 取lag天前，窗口大小为win的日期的数据【i.e.取两个时间节点间的数据】\n",
        "                df_window = sale_data[(sale_data.date <= day-timedelta(days=lag)) & (sale_data.date > day-timedelta(days=lag+win))]\n",
        "                if lag == 28:\n",
        "                    # 将数据按id聚合\n",
        "                    df_window_grouped = df_window.groupby(\"id\").agg({'sales':['mean','median','std','max']})\n",
        "                    df_window_grouped = df_window_grouped.reindex(sale_data.loc[sale_data.date==day,'id'])['sales']\n",
        "                    sale_data[[\n",
        "                            f\"rmean_{lag}_{win}\",\n",
        "                            f\"rmedian_{lag}_{win}\",\n",
        "                            f\"rstd_{lag}_{win}\",\n",
        "                            f\"rmax_{lag}_{win}\",\n",
        "                            ]] = df_window_grouped.set_index(sale_data.loc[sale_data.date == day].index).astype('float32')\n",
        "                else:\n",
        "                    df_window_grouped = df_window.groupby(\"id\").agg({'sales':'mean'})\n",
        "                    df_window_grouped = df_window_grouped.reindex(sale_data.loc[sale_data.date==day,'id'])['sales']\n",
        "                    sale_data.loc[sale_data.date == day, f\"rmean_{lag}_{win}\"] = df_window_grouped.values\n",
        "\n",
        "    return sale_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVLQ56uQQg2p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed_all()\n",
        "sale_data = create_train_data(1800)\n",
        "sale_data = create_feature(sale_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztxC_d_0SSj-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "USELESS_COLS = [\"id\", \"date\", \"sales\", \"d\"]\n",
        "TARGET = 'sales'\n",
        "CAT_COLS = ['item_id', 'dept_id','store_id', 'cat_id', 'state_id','event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
        "TRAIN_COLS = sale_data.columns[~sale_data.columns.isin(USELESS_COLS)].tolist()\n",
        "X_valid = sale_data.loc[(sale_data.date <= '2016-04-24') & (sale_data.date > '2016-03-27'),TRAIN_COLS]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yAxGMdJTT6r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "1e15c382-57f0-492f-9da9-02b5f98f7bdb"
      },
      "source": [
        "X_valid.columns"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['item_id', 'dept_id', 'store_id', 'cat_id', 'state_id', 'wday', 'month',\n",
              "       'year', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2',\n",
              "       'snap_CA', 'snap_TX', 'snap_WI', 'sell_price', 'week', 'quarter',\n",
              "       'mday', 'lag_7', 'lag_28', 'rmean_7_7', 'rmean_28_7', 'rmedian_28_7',\n",
              "       'rstd_28_7', 'rmax_28_7', 'rmean_7_28', 'rmean_28_28', 'rmedian_28_28',\n",
              "       'rstd_28_28', 'rmax_28_28'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRXYOZuiSSC9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lgb_v6 = lgb.Booster(model_file='lgb_v6.txt')\n",
        "score_v6 = evaluate(lgb_v6,X_valid)\n",
        "fi_test(lgb_v6,X_valid,score_v6,X_valid.columns.tolist()[-12:])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}